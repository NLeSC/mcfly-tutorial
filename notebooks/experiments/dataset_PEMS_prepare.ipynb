{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: PEMS\n",
    "\n",
    "http://www.timeseriesclassification.com/description.php?Dataset=PEMS-SF  \n",
    "https://archive.ics.uci.edu/ml/datasets/PEMS-SF\n",
    "\n",
    "### Info from data source:\n",
    "Source: California Department of Transportation, www.pems.dot.ca.gov\n",
    "Creator: Marco Cuturi, Kyoto University, mcuturi '@' i.kyoto-u.ac.jp\n",
    "\n",
    "Data Set Information:\n",
    "\n",
    "15 months worth of daily data from the California Department of Transportation PEMS website. The data describes the occupancy\n",
    "rate, between 0 and 1, of different car lanes of San Francisco bay area freeways. The measurements cover the period from Jan. 1st 2008 to Mar. 30th 2009 and are sampled every 10 minutes. We consider each day in this database as a single time series of dimension 963 (the number of sensors which functioned consistently throughout the studied period) and length 6 x 24=144. We remove public holidays from the dataset, as well\n",
    "as two days with anomalies (March 8th 2009 and March 9th 2008) where all sensors were muted between 2:00 and 3:00 AM.\n",
    "This results in a database of 440 time series.\n",
    "\n",
    "The task is to classify each observed day as the correct day of the week, from Monday to Sunday, e.g. label it with an integer in {1,2,3,4,5,6,7}.\n",
    "Each attribute describes the measurement of the occupancy rate (between 0 and 1) of a captor location as recorded by a measuring station, at a given timestamp in time during the day. The ID of each station is given in the stations_list text file. For more information on the location (GPS, Highway, Direction) of each station please refer to the PEMS website. There are 963 (stations) x 144 (timestamps) = 138.672 attributes for each record.\n",
    "\n",
    "Relevant Papers:\n",
    "[1] M. Cuturi, Fast Global Alignment Kernels, Proceedings of the Intern. Conference on Machine Learning 2011.\n",
    "\n",
    "\n",
    "### Size:\n",
    "+ Training samples: 267\n",
    "+ Test sampels: 173\n",
    "+ Dimension: 144 timepoints x 963 channels\n",
    "+ Classes: 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "CODE = 'C:\\\\OneDrive - Netherlands eScience Center\\\\Project_mcfly\\\\mcfly\\\\mcfly'\n",
    "DATA = 'C:\\\\OneDrive - Netherlands eScience Center\\\\Project_mcfly\\\\data\\\\PEMS-SF'\n",
    "sys.path.append(CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train = os.path.join(DATA, 'PEMS-SF_TRAIN.arff')\n",
    "file_test = os.path.join(DATA, 'PEMS-SF_TEST.arff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arff(filename):\n",
    "    start = 0\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    start_line = 0\n",
    "    with open(filename) as fp:\n",
    "        line = fp.readline()\n",
    "        count = 0\n",
    "        while line:\n",
    "            if start == 1:\n",
    "                label = line.split(\"',\")[-1]\n",
    "                labels.append(label.replace('\\n', ''))\n",
    "                line = line.split(\"',\")[0] \n",
    "                lines = line.split('\\\\n')\n",
    "                data_line = []\n",
    "                for l in lines:\n",
    "                    data_line_sub = []\n",
    "                    #for entry in l.split(','):\n",
    "                        #data_line_sub.append(entry.replace(\"'\", \"\"))\n",
    "                    #data_line.append(data_line_sub)\n",
    "                    data_line.append([x.replace(\"'\", \"\") for x in l.split(',')])\n",
    "                data.append(data_line)\n",
    "\n",
    "            if line.startswith('@data'):\n",
    "                start_line = count\n",
    "                #print(\"Actual data start in line\", start_line)\n",
    "                start = 1\n",
    "\n",
    "            line = fp.readline()\n",
    "            count += 1\n",
    "            \n",
    "    return np.swapaxes(np.array(data).astype(float), 1,2), labels\n",
    "\n",
    "X_train, y_train = load_arff(file_train)\n",
    "X_test0, y_test0 = load_arff(file_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (267, 144, 963)\n",
      "267\n",
      "X_test.shape (173, 144, 963)\n",
      "173\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(len(y_train))\n",
    "\n",
    "print(\"X_test.shape\", X_test0.shape)\n",
    "print(len(y_test0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0134, 0.0129, 0.0122, 0.0105, 0.0103, 0.0095, 0.0086, 0.0084,\n",
       "       0.0079, 0.0075, 0.0075, 0.0076, 0.0073, 0.0073, 0.007 , 0.0074,\n",
       "       0.0074, 0.0072, 0.0071, 0.0078, 0.0078, 0.0101, 0.0109, 0.0111,\n",
       "       0.0113, 0.0126, 0.0161, 0.0175, 0.0238, 0.0247, 0.0275, 0.0314,\n",
       "       0.0397, 0.0532, 0.0568, 0.0593, 0.0589, 0.0721, 0.0765, 0.0893,\n",
       "       0.0947, 0.0951, 0.094 , 0.0987, 0.1094, 0.1108, 0.1159, 0.1143,\n",
       "       0.1076, 0.1083, 0.1078, 0.1052, 0.1051, 0.0975, 0.0931, 0.0879,\n",
       "       0.086 , 0.0861, 0.0857, 0.0834, 0.0754, 0.0745, 0.0736, 0.0731,\n",
       "       0.0742, 0.0725, 0.0691, 0.0704, 0.0711, 0.072 , 0.0713, 0.0699,\n",
       "       0.0683, 0.0703, 0.0707, 0.0714, 0.0719, 0.0718, 0.0683, 0.0703,\n",
       "       0.071 , 0.0703, 0.0723, 0.0706, 0.0698, 0.072 , 0.0736, 0.0744,\n",
       "       0.0774, 0.0743, 0.0731, 0.079 , 0.079 , 0.077 , 0.0814, 0.0794,\n",
       "       0.0759, 0.0791, 0.0769, 0.0765, 0.0823, 0.081 , 0.0813, 0.0865,\n",
       "       0.0892, 0.0834, 0.083 , 0.0789, 0.0755, 0.0747, 0.0723, 0.0657,\n",
       "       0.0659, 0.0619, 0.0554, 0.0543, 0.0509, 0.0493, 0.046 , 0.0446,\n",
       "       0.0413, 0.0419, 0.0417, 0.0391, 0.0383, 0.0374, 0.0376, 0.0399,\n",
       "       0.0406, 0.038 , 0.0374, 0.0359, 0.0336, 0.0335, 0.03  , 0.0294,\n",
       "       0.0274, 0.0254, 0.0219, 0.0218, 0.0203, 0.0179, 0.0179, 0.0146])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0,:,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split test into test and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 25\n",
      "3.0 26\n",
      "1.0 30\n",
      "4.0 23\n",
      "7.0 20\n",
      "5.0 22\n",
      "6.0 27\n"
     ]
    }
   ],
   "source": [
    "y_val = []\n",
    "y_test = []\n",
    "IDs_val = []\n",
    "IDs_test = []\n",
    "\n",
    "np.random.seed(1)\n",
    "for label in list(set(y_test0)):\n",
    "    idx = np.where(np.array(y_test0) == label)[0]\n",
    "    idx1 = np.random.choice(idx, len(idx)//2, replace=False)\n",
    "    idx2 = list(set(idx) - set(idx1))\n",
    "    IDs_val.extend(idx1)\n",
    "    IDs_test.extend(idx2)\n",
    "    y_val.extend(len(idx1) * [label])\n",
    "    y_test.extend(len(idx2) * [label])\n",
    "\n",
    "    print(label, y_test0.count(label))\n",
    "    \n",
    "X_test = X_test0[IDs_test,:,:]\n",
    "X_val = X_test0[IDs_val,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88, 144, 963) (85, 144, 963)\n",
      "88 85\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape, X_val.shape)\n",
    "print(len(y_test), len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save pre-processed data as numpy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'PEMS_'\n",
    "\n",
    "output_path = 'C:\\\\OneDrive - Netherlands eScience Center\\\\Project_mcfly\\\\data\\\\processed'\n",
    "np.save(os.path.join(output_path, dataset_name + 'X_train.npy'), X_train)\n",
    "np.save(os.path.join(output_path, dataset_name + 'X_val.npy'), X_val)\n",
    "np.save(os.path.join(output_path, dataset_name + 'X_test.npy'), X_test)\n",
    "np.save(os.path.join(output_path, dataset_name + 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(output_path, dataset_name + 'y_val.npy'), y_val)\n",
    "np.save(os.path.join(output_path, dataset_name + 'y_test.npy'), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or: Create new split of data ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
