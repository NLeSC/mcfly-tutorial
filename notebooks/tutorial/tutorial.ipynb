{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial PAMAP2 with mcfly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to get you familiar with training Neural Networks for time series using mcfly. At the end of the tutorial, you will have compared several Neural Network architectures you know how to train the best performing network.\n",
    "\n",
    "As an example dataset we use the publicly available [PAMAP2 dataset](https://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring). It contains time series data from movement sensors worn by nine individuals. The data is labelled with the activity types that these individuals did and the aim is to train and evaluate a *classifier*.\n",
    "\n",
    "Before you can start, please make sure you install mcfly (see the [mcfly installation page](https://github.com/NLeSC/mcfly))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 16:11:08.454959: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-14 16:11:08.540349: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-14 16:11:08.542850: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-14 16:11:08.542861: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-14 16:11:08.939188: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-14 16:11:08.939224: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-14 16:11:08.939228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# mcfly\n",
    "import mcfly\n",
    "import tensorflow as tf\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "from utils import tutorial_pamap2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data pre-procesed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a function for you to fetch the preprocessed data from https://zenodo.org/record/834467. Please specify the `directory_to_extract_to` in the code below and then execute the cell. This will download the preprocessed data into the directory in the `data` subdirectory. The output of the function is the path where the preprocessed data was stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify in which directory you want to store the data:\n",
    "directory_to_extract_to = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data...\n",
      "Extracting data...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "data_path = tutorial_pamap2.download_preprocessed_data(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit about the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [PAMAP2 dataset](https://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring) contains data from three movement sensors worn on the hand, ankle and chest by nine test subjects. These subjects performed a protocol of several activities.\n",
    "\n",
    "Each sensor has three channels (acceleration on x, y and z axes). This gives us, for each time step, 9 values. The data is recorded on 100 Hz.\n",
    "\n",
    "The preprocessed data is split into smaller segments with a window of 512 time steps, corresponding to 5.12 seconds. We only include segments that completely fall into one activity period: the activity is the *label* of the segment.\n",
    "\n",
    "The goal of classification is to assign an activity label to an previously unseen segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pre-processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the preprocessed data as stored in Numpy-files. Please note that the data has already been split up in a training (\\~91%), validation (\\~1%), and test (\\~8%) subsets. It is common practice to call the input data X and the labels y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train_binary, X_val, y_val_binary, X_test, y_test_binary, labels = tutorial_pamap2.load_data(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data X and labels y are of type Numpy array. In the cell below we inspect the shape of the data. As you can see the shape of X is expressed as a Python tuple containing: the number of samples, length of the time series, and the number of channels for each sample. Similarly, the shape of y is represents the number of samples and the number of classes (unique labels). Note that y has the format of a binary array where only the correct class for each sample is assigned a 1. This is called one-hot-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X training shape: (11397, 512, 9)\n",
      "y training shape: (11397, 7)\n",
      "y validation shape: (100, 7)\n",
      "y test shape: (1000, 7)\n"
     ]
    }
   ],
   "source": [
    "print('X training shape:', X_train.shape)\n",
    "print('y training shape:', y_train_binary.shape)\n",
    "print('y validation shape:', y_val_binary.shape)\n",
    "print('y test shape:', y_test_binary.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is split between train test and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the distribution of the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lying</th>\n",
       "      <td>0.136615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sitting</th>\n",
       "      <td>0.130736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standing</th>\n",
       "      <td>0.136703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walking</th>\n",
       "      <td>0.176625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cycling</th>\n",
       "      <td>0.118540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaccuum_cleaning</th>\n",
       "      <td>0.125208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ironing</th>\n",
       "      <td>0.175573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  frequency\n",
       "lying              0.136615\n",
       "sitting            0.130736\n",
       "standing           0.136703\n",
       "walking            0.176625\n",
       "cycling            0.118540\n",
       "vaccuum_cleaning   0.125208\n",
       "ironing            0.175573"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies = y_train_binary.mean(axis=0)\n",
    "frequencies_df = pd.DataFrame(frequencies, index=labels, columns=['frequency'])\n",
    "frequencies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Question 1: How many channels does this dataset have?*\n",
    "### *Question 2: What is the least common activity label in this dataset?*\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step in the development of any deep learning model is to create a model architecture. As we do not know what architecture is best for our data we will create a set of random models to investigate which architecture is most suitable for our data and classification task. This process, creating random models, checking how good they are and then selecting the best one is called a 'random search'. A random search is considered to be the most robust approach to finding a good model. You will need to specificy how many models you want to create with argument 'number_of_models'. See for a full overview of the optional arguments the function documentation of modelgen.generate_models by running `modelgen.generate_models?`.\n",
    "\n",
    "##### What number of models to select?\n",
    "This number differs per dataset. More models will give better results but it will take longer to evaluate them. For the purpose of this tutorial we recommend trying only 2 models to begin with. If you have enough time you can try a larger number of models, e.g. 10 or 20 models. Because mcfly uses random search, you will get better results when using more models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olga/Documents/mcfly-tutorial/env/lib/python3.10/site-packages/mcfly/modelgen.py:86: UserWarning: Specified number_of_models is smaller than the given number of model types.\n",
      "  warnings.warn(\"Specified number_of_models is smaller than the given number of model types.\")\n",
      "2022-12-14 16:29:30.817632: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-14 16:29:30.817649: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-14 16:29:30.817662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (XPS139310): /proc/driver/nvidia/version does not exist\n",
      "2022-12-14 16:29:30.817815: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "num_classes = y_train_binary.shape[1]\n",
    "metric = 'accuracy'\n",
    "models = mcfly.modelgen.generate_models(X_train.shape,\n",
    "                                  number_of_classes=num_classes,\n",
    "                                  number_of_models = 2,\n",
    "                                       metrics=[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the models\n",
    "We can have a look at the models that were generated. The layers are shown as table rows. Most common layer types are 'Convolution' and 'LSTM' and 'Dense'. For more information see the [mcfly user manual](https://github.com/NLeSC/mcfly/wiki/User-manual) and the [tutorial cheat sheet](https://github.com/NLeSC/mcfly-tutorial/blob/master/cheatsheet.md). The summary also shows the data shape of each layer output and the number of parameters that are trained within this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------\n",
      "Model 0 \n",
      "\n",
      "Model type: ResNet \n",
      "\n",
      "Hyperparameters:\n",
      "{'learning_rate': 0.0022439468517196116, 'regularization_rate': 0.004943480463153602, 'network_depth': 2, 'min_filters_number': 39, 'max_kernel_size': 10}\n",
      " \n",
      "Model description:\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 512, 9)]     0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 512, 9)      36          ['input_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 512, 39)      3549        ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 512, 39)     156         ['conv1d[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 512, 39)      0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 512, 39)      15249       ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 512, 39)     156         ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 512, 39)      0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 512, 39)      15249       ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 512, 39)     156         ['conv1d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 512, 39)      0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 512, 39)      1560        ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 512, 39)      0           ['re_lu_2[0][0]',                \n",
      "                                                                  'conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 512, 54)      14796       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 512, 54)     216         ['conv1d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 512, 54)      0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 512, 54)      20466       ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 512, 54)     216         ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 512, 54)      0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 512, 54)      20466       ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 512, 54)     216         ['conv1d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 512, 54)      0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 512, 54)      2970        ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 512, 54)      0           ['re_lu_5[0][0]',                \n",
      "                                                                  'conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 54)          0           ['add_1[0][0]']                  \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 7)            385         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 95,842\n",
      "Trainable params: 95,266\n",
      "Non-trainable params: 576\n",
      "__________________________________________________________________________________________________\n",
      " \n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Model 1 \n",
      "\n",
      "Model type: InceptionTime \n",
      "\n",
      "Hyperparameters:\n",
      "{'learning_rate': 0.03441279764432193, 'regularization_rate': 0.0008005932532411755, 'network_depth': 6, 'filters_number': 95, 'max_kernel_size': 41}\n",
      " \n",
      "Model description:\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 512, 9)]     0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 512, 9)      36          ['input_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv1d_8 (Conv1D)              (None, 512, 32)      288         ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 512, 9)       0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 512, 95)      124640      ['conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 512, 95)      60800       ['conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 512, 95)      30400       ['conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 512, 95)      855         ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 512, 380)     0           ['conv1d_9[0][0]',               \n",
      "                                                                  'conv1d_10[0][0]',              \n",
      "                                                                  'conv1d_11[0][0]',              \n",
      "                                                                  'conv1d_12[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 512, 380)    1520        ['concatenate[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 512, 380)     0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 512, 32)      12160       ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 512, 380)    0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 512, 95)      124640      ['conv1d_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 512, 95)      60800       ['conv1d_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 512, 95)      30400       ['conv1d_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 512, 95)      36100       ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 512, 380)     0           ['conv1d_14[0][0]',              \n",
      "                                                                  'conv1d_15[0][0]',              \n",
      "                                                                  'conv1d_16[0][0]',              \n",
      "                                                                  'conv1d_17[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 512, 380)    1520        ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 512, 380)     0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 512, 32)      12160       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 512, 380)    0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)             (None, 512, 95)      124640      ['conv1d_18[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)             (None, 512, 95)      60800       ['conv1d_18[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)             (None, 512, 95)      30400       ['conv1d_18[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)             (None, 512, 95)      36100       ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 512, 380)     0           ['conv1d_19[0][0]',              \n",
      "                                                                  'conv1d_20[0][0]',              \n",
      "                                                                  'conv1d_21[0][0]',              \n",
      "                                                                  'conv1d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)             (None, 512, 380)     3420        ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 512, 380)    1520        ['concatenate_2[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 512, 380)    1520        ['conv1d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 512, 380)     0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 512, 380)     0           ['batch_normalization_11[0][0]', \n",
      "                                                                  'activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 512, 380)     0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)             (None, 512, 32)      12160       ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 512, 380)    0           ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)             (None, 512, 95)      124640      ['conv1d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_26 (Conv1D)             (None, 512, 95)      60800       ['conv1d_24[0][0]']              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv1d_27 (Conv1D)             (None, 512, 95)      30400       ['conv1d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_28 (Conv1D)             (None, 512, 95)      36100       ['max_pooling1d_3[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 512, 380)     0           ['conv1d_25[0][0]',              \n",
      "                                                                  'conv1d_26[0][0]',              \n",
      "                                                                  'conv1d_27[0][0]',              \n",
      "                                                                  'conv1d_28[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 512, 380)    1520        ['concatenate_3[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 512, 380)     0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_29 (Conv1D)             (None, 512, 32)      12160       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 512, 380)    0           ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_30 (Conv1D)             (None, 512, 95)      124640      ['conv1d_29[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_31 (Conv1D)             (None, 512, 95)      60800       ['conv1d_29[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_32 (Conv1D)             (None, 512, 95)      30400       ['conv1d_29[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_33 (Conv1D)             (None, 512, 95)      36100       ['max_pooling1d_4[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 512, 380)     0           ['conv1d_30[0][0]',              \n",
      "                                                                  'conv1d_31[0][0]',              \n",
      "                                                                  'conv1d_32[0][0]',              \n",
      "                                                                  'conv1d_33[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 512, 380)    1520        ['concatenate_4[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 512, 380)     0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_34 (Conv1D)             (None, 512, 32)      12160       ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 512, 380)    0           ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_35 (Conv1D)             (None, 512, 95)      124640      ['conv1d_34[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_36 (Conv1D)             (None, 512, 95)      60800       ['conv1d_34[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_37 (Conv1D)             (None, 512, 95)      30400       ['conv1d_34[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_38 (Conv1D)             (None, 512, 95)      36100       ['max_pooling1d_5[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 512, 380)     0           ['conv1d_35[0][0]',              \n",
      "                                                                  'conv1d_36[0][0]',              \n",
      "                                                                  'conv1d_37[0][0]',              \n",
      "                                                                  'conv1d_38[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_39 (Conv1D)             (None, 512, 380)     144400      ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 512, 380)    1520        ['concatenate_5[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 512, 380)    1520        ['conv1d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 512, 380)     0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 512, 380)     0           ['batch_normalization_15[0][0]', \n",
      "                                                                  'activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 512, 380)     0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 380)         0           ['activation_7[0][0]']           \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 7)            2667        ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,700,166\n",
      "Trainable params: 1,694,068\n",
      "Non-trainable params: 6,098\n",
      "__________________________________________________________________________________________________\n",
      " \n"
     ]
    }
   ],
   "source": [
    "models_to_print = range(len(models))\n",
    "for i, item in enumerate(models):\n",
    "    if i in models_to_print:\n",
    "        model, params, model_types = item\n",
    "        print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Model \" + str(i), '\\n')          \n",
    "        print(\"Model type:\", model_types, '\\n')        \n",
    "        print(\"Hyperparameters:\")\n",
    "        print(params)\n",
    "        print(\" \")\n",
    "        print(\"Model description:\")\n",
    "        model.summary()        \n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Question 3: Can you guess what hyperparameter 'learning rate' stands for?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models\n",
    "Now that the model architectures have been generated it is time to compare the models by training them on a subset of the training data and evaluating the models on the validation subset. This will help us to choose the best candidate model. The performance results for the models are stored in a json file, which we will visually inspect later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory where the results, e.g. json file, will be stored\n",
    "resultpath = os.path.join(directory_to_extract_to, 'data/models')\n",
    "if not os.path.exists(resultpath):\n",
    "        os.makedirs(resultpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to train each of the models that we generated. On the one hand we want to train them as quickly as possible in order to be able to pick the best one as soon as possible. On the other hand we have to train each model long enough to get a good impression of its potential.\n",
    "\n",
    "We can influence the train time by adjusting the number of data samples that are used. This can be set with the argument 'subset_size'. We can also adjust the number of times the subset is iterated over. This number is called an epoch. We recommend to start with no more than 5 epochs and a maximum subset size of 300. You can experiment with these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated models will be trained on subset of the data (subset size: 300).\n",
      "Training model 0 ResNet\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olga/Documents/mcfly-tutorial/env/lib/python3.10/site-packages/mcfly/find_architecture.py:134: UserWarning: Argument 'metric' is deprecated and will be ignored.\n",
      "  warnings.warn(\"Argument 'metric' is deprecated and will be ignored.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 3s 52ms/step - loss: 1.1598 - accuracy: 0.6000 - val_loss: 4.2501 - val_accuracy: 0.2800\n",
      "Epoch 2/5\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.6305 - accuracy: 0.8200 - val_loss: 7.1359 - val_accuracy: 0.2300\n",
      "Epoch 2: early stopping\n",
      "Training model 1 InceptionTime\n",
      "Epoch 1/5\n",
      "15/15 [==============================] - 10s 491ms/step - loss: 0.9077 - accuracy: 0.7300 - val_loss: 79.5460 - val_accuracy: 0.2200\n",
      "Epoch 2/5\n",
      "15/15 [==============================] - 7s 501ms/step - loss: 0.4192 - accuracy: 0.8400 - val_loss: 276.1582 - val_accuracy: 0.1700\n",
      "Epoch 2: early stopping\n",
      "Details of the training process were stored in  ./data/models/modelcomparison.json\n"
     ]
    }
   ],
   "source": [
    "outputfile = os.path.join(resultpath, 'modelcomparison.json')\n",
    "histories, val_accuracies, val_losses = mcfly.find_architecture.train_models_on_samples(X_train, y_train_binary,\n",
    "                                                                           X_val, y_val_binary,\n",
    "                                                                           models,nr_epochs=5,\n",
    "                                                                           subset_size=300,\n",
    "                                                                           verbose=True,\n",
    "                                                                           outputfile=outputfile,\n",
    "                                                                            metric=metric)\n",
    "print('Details of the training process were stored in ',outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Question 4: What does the term 'loss' in the output refer to?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect model performance (Visualization)\n",
    "\n",
    "We can inspect the learning process in the visualization tool on http://nlesc.github.io/mcfly/.\n",
    "\n",
    "Alternatively, you can run the visualization from a local web service:\n",
    "- Clone the mcfly github repository (if you haven't done so already for visualization)\n",
    "\n",
    " `git clone https://github.com/NLeSC/mcfly`\n",
    "\n",
    "\n",
    "- navigate to the html folder:\n",
    "\n",
    " `cd mcfly/html`\n",
    "\n",
    "\n",
    "- Start a web server. This can be done in various ways, for example:\n",
    " - for python 3 you can use: `python3 -m http.server`\n",
    " - for python 2 you can use: `python2 -m SimpleHTTPServer`\n",
    "\n",
    "Notice the port number the web server is serving on. This is usually 8000.\n",
    "With a web browser, navigate to [localhost:8000](localhost:8000). \n",
    "\n",
    "You need to upload the json file that contains the details of the training process. The following line of code shows the path to this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/models/modelcomparison.json'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Question 5:  Look at the visualization. Which model performs best?*\n",
    "### *Question 6:  Did you train all models with a sufficient number of iterations?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect model performance (table)\n",
    "\n",
    "Let's compare the performance of the models by showing the results as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'learning_rate': 0.0022439468517196116, 'regu...</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.630516</td>\n",
       "      <td>0.23</td>\n",
       "      <td>7.135896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'learning_rate': 0.03441279764432193, 'regula...</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.419189</td>\n",
       "      <td>0.17</td>\n",
       "      <td>276.158173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  train_accuracy  \\\n",
       "0  {'learning_rate': 0.0022439468517196116, 'regu...            0.82   \n",
       "1  {'learning_rate': 0.03441279764432193, 'regula...            0.84   \n",
       "\n",
       "   train_loss  val_accuracy    val_loss  \n",
       "0    0.630516          0.23    7.135896  \n",
       "1    0.419189          0.17  276.158173  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = 'accuracy'\n",
    "modelcomparisons = pd.DataFrame({'model':[str(params) for model, params, model_types in models],\n",
    "                       'train_{}'.format(metric): [history.history[metric][-1] for history in histories],\n",
    "                       'train_loss': [history.history['loss'][-1] for history in histories],\n",
    "                       'val_{}'.format(metric): [history.history['val_{}'.format(metric)][-1] for history in histories],\n",
    "                       'val_loss': [history.history['val_loss'][-1] for history in histories]\n",
    "                       })\n",
    "modelcomparisons.to_csv(os.path.join(resultpath, 'modelcomparisons.csv'))\n",
    "\n",
    "modelcomparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Choose the best model\n",
    "Now that we found an effective architecture, we can choose the most promising model. For example, we can choose the model with the highest accuracy on the validation data set. To maximize this models performance, we will train this model on more data and more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type and parameters of the best model:\n",
      "ResNet\n",
      "{'learning_rate': 0.0022439468517196116, 'regularization_rate': 0.004943480463153602, 'network_depth': 2, 'min_filters_number': 39, 'max_kernel_size': 10}\n"
     ]
    }
   ],
   "source": [
    "best_model_index = np.argmax(val_accuracies)\n",
    "best_model, best_params, best_model_types = models[best_model_index]\n",
    "print('Model type and parameters of the best model:')\n",
    "print(best_model_types)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the best model on the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified the best model architecture out of our random pool of models we can continue by training the model on the full training set.\n",
    "\n",
    "This would take some time, so instead we will train  on only a slightly larger subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 2s 69ms/step - loss: 0.3706 - accuracy: 0.8820 - val_loss: 6.5006 - val_accuracy: 0.3200\n"
     ]
    }
   ],
   "source": [
    "#We make a copy of the model, to start training from fresh\n",
    "nr_epochs = 1\n",
    "datasize = 500 # Change in `X_train.shape[0]` if training complete data set\n",
    "history = best_model.fit(X_train[:datasize,:,:], y_train_binary[:datasize,:],\n",
    "              epochs=nr_epochs, validation_data=(X_val, y_val_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Question 7: Why does it take longer to train the best model for one epoch now than when we were  comparing model archicatures earlier on?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Question 8: Do you think it is useful to train with more than 1 epoch?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving, loading and comparing reloaded model with original model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be saved for future use. The savemodel function will save two separate files: a json file for the architecture and a npy (numpy array) file for the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'my_bestmodel.h5'\n",
    "model_path = os.path.join(resultpath,modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChristiaanMeijer\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    }
   ],
   "source": [
    "best_model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reloaded = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been reloaded. Let's reassure that this model has the same weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all([np.all(x==y) for x,y in zip(best_model.get_weights(), model_reloaded.get_weights())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now dive further into the Neural network that we created.\n",
    "We provide here a network that has been trained on the complete train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('./model/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the objects `models`, `best_model_fullytrained` and `best_model` that resulted from the mcfly functions are Keras objects. This means that you can use Keras functions on the objects, for example  `.predict`, (which when given the data, outputs the predictions for each sample) and `.evaluate` (which when given the data and the labels computes how well this model performs) . These functions are all documented in the [Keras documentation](https://www.tensorflow.org/api_docs/python/tf/keras). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect model predictions on validation data\n",
    "datasize = X_val.shape[0]\n",
    "probs = model.predict(X_val[:datasize,:,:],batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lying</th>\n",
       "      <th>sitting</th>\n",
       "      <th>standing</th>\n",
       "      <th>walking</th>\n",
       "      <th>cycling</th>\n",
       "      <th>vaccuum_cleaning</th>\n",
       "      <th>ironing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lying</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sitting</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walking</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cycling</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaccuum_cleaning</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ironing</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  lying  sitting  standing  walking  cycling  \\\n",
       "lying                13        0         0        0        0   \n",
       "sitting               0       13         1        0        0   \n",
       "standing              0        0        20        0        0   \n",
       "walking               0        0         0       15        0   \n",
       "cycling               0        0         0        0       12   \n",
       "vaccuum_cleaning      0        0         0        0        0   \n",
       "ironing               0        1         0        0        1   \n",
       "\n",
       "                  vaccuum_cleaning  ironing  \n",
       "lying                            0        0  \n",
       "sitting                          0        0  \n",
       "standing                         0        0  \n",
       "walking                          0        0  \n",
       "cycling                          0        0  \n",
       "vaccuum_cleaning                 8        0  \n",
       "ironing                          0       16  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#columns are predicted, rows are truth\n",
    "predicted = probs.argmax(axis=1)\n",
    "y_index = y_val_binary.argmax(axis=1)\n",
    "confusion_matrix = pd.crosstab(pd.Series(y_index), pd.Series(predicted))\n",
    "confusion_matrix.index = [labels[i] for i in confusion_matrix.index]\n",
    "confusion_matrix.columns = [labels[i] for i in confusion_matrix.columns]\n",
    "confusion_matrix.reindex(columns=[l for l in labels], fill_value=0)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4157 - accuracy: 0.9620\n",
      "Score of best model: [0.4156666696071625, 0.9620000123977661]\n"
     ]
    }
   ],
   "source": [
    "## Test on Testset\n",
    "score_test = model.evaluate(X_test, y_test_binary, verbose=True)\n",
    "print('Score of best model: ' + str(score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
